{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3\n",
    "## Monday June 3, 2019:\n",
    "To start out today, I decided to make a smaller .csv file to test the python canopy clustering code. I used Multigenome_CSV_Generator.py to generate one large .csv file of counts of the top 10 kmers from the previously generated .csv files from 10 genomes. I then tried to run my python code for canopy clustering. While running, the .csv file stored in a data frame used about 16 GB of RAM, and the CPU load of my mac grew sharply when trying to calculate the distance matrix. I was able to get lots of small clusters since the cluster cut-off values were lower than the distance values from the Euclidean metric. I then switched the distance metric to cosine similarity (range -1 to 1) and tried to find a good cut-off value for canopy inclusion. Once I had good clustering, I tried to generate a list of groupings to import into FindMyFriends using the manualGrouping() method. For this, I generated a list where the list index was a protein, and the value of the list at that index was the grouping of the protein. Once I examined the grouping list, I then realized that the code had an error; points were being clustered multiple times. I fixed that issue and finalized the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from gdbasset canopy.py gist on github: https://gist.github.com/gdbassett/528d816d035f2deaaca1\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# X shoudl be a numpy matrix, very likely sparse matrix: http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix\n",
    "# T1 > T2 for overlapping clusters\n",
    "# T1 = Distance to centroid point to not include in other clusters\n",
    "# T2 = Distance to centroid point to include in cluster\n",
    "# T1 > T2 for overlapping clusters\n",
    "# T1 < T2 will have points which reside in no clusters\n",
    "# T1 == T2 will cause all points to reside in mutually exclusive clusters\n",
    "# Distance metric can be any from here: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html\n",
    "# filemap may be a list of point names in their order in X. If included, row numbers from X will be replaced with names from filemap. \n",
    " \n",
    "def canopy(X1_dist, T1, T2, filemap=None):\n",
    "    canopies = dict()\n",
    "    canopy_points = set(range(X1_dist.shape[0]))\n",
    "    grouping_list = [None] * X1_dist.shape[0]\n",
    "    iteration = 0\n",
    "    clustered_points = []\n",
    "    while canopy_points:\n",
    "        iteration = iteration + 1\n",
    "        print(\"iteration: \" + str(iteration))\n",
    "        point = canopy_points.pop()\n",
    "        i = len(canopies)\n",
    "        points_in_T2 = np.where(X1_dist[point] < T2)[0]\n",
    "        points_to_cluster = set(points_in_T2).difference(set(clustered_points))\n",
    "        canopies[i] = {\"c\":point, \"points\": points_to_cluster}\n",
    "        clustered_points.extend(points_to_cluster)\n",
    "        canopy_points = canopy_points.difference(set(np.where(X1_dist[point] < T1)[0]))\n",
    "        \n",
    "        for entry in canopies[i][\"points\"]:\n",
    "            grouping_list[entry] = point\n",
    "        \n",
    "    if filemap:\n",
    "        for canopy_id in canopies.keys():\n",
    "            canopy = canopies.pop(canopy_id)\n",
    "            canopy2 = {\"c\":filemap[canopy['c']], \"points\":list()}\n",
    "            for point in canopy['points']:\n",
    "                canopy2[\"points\"].append(filemap[point])\n",
    "            canopies[canopy_id] = canopy2\n",
    "    return grouping_list\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv('/Users/matthewthompson/Documents/UAMS_SURF/K-mer_testing/FAA_files/10_genome_k4_kmer_top_10_table.csv')\n",
    "    #gets protein_ids\n",
    "    kmers = list(df['Unnamed: 0'])\n",
    "    df['kmers'] = kmers\n",
    "    #move from data column to index column\n",
    "    df.set_index('kmers',inplace = True)\n",
    "    #delete protein header list\n",
    "    del df['Unnamed: 0']\n",
    "    df = df.T\n",
    "    df.head(3)\n",
    "    print(\"canopy clustering:\")\n",
    "    X1_dist = pairwise_distances(df.to_numpy(), metric='cosine')\n",
    "    \n",
    "    grouping_list = canopy(X1_dist, 0.99, 0.99)\n",
    "    \n",
    "    '''\n",
    "    with open('cosine_canopy_clusters_0.99.csv', 'w') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        for key, value in clusters.items():\n",
    "            writer.writerow([key, value])\n",
    "    '''\n",
    "    with open('grouping_list.csv', 'w') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(grouping_list) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuesday June 4, 2019:\n",
    "To recap, what I have so far:\n",
    "* download genomes from GenBank using Batch Entrez \n",
    "* use prodigal to convert .fasta files to .faa files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prodigal -i inputfilename.fna -a outputfilename.faa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* generate .csv files of amino acid kmer counts (using Dayhoff compressed alphabet) for each genome (kmer_counter_test1.R)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"ape\")\n",
    "library(\"kmer\")\n",
    "library(\"MASS\")\n",
    "setwd(\"/Users/matthewthompson/Documents/UAMS_SURF/K-mer_testing/FAA_files\")\n",
    "\n",
    "\n",
    "file_list <- c(\"GCA_000005845.2_ASM584v2\",\"GCA_000006665.1_ASM666v1\",\"GCA_000006925.2_ASM692v2\",\"GCA_000007445.1_ASM744v1\",\"GCA_000008865.2_ASM886v2\",\"GCA_000010245.1_ASM1024v1\",\"GCA_000010485.1_ASM1048v1\",\"GCA_000012005.1_ASM1200v1\",\"GCA_000012025.1_ASM1202v1\",\"GCA_000019645.1_ASM1964v1\",\"GCA_000026325.2_ASM2632v2\",\"GCA_000091005.1_ASM9100v1\",\"GCA_000092525.1_ASM9252v1\",\"GCA_000167875.1_ASM16787v1\",\"GCA_000176575.2_ASM17657v2\",\"GCA_000176655.2_ASM17665v2\",\"GCA_000176695.2_ASM17669v2\",\"GCA_000333215.1_ASM33321v1\",\"GCA_000350785.1_Esch_coli_KTE21_V1\",\"GCA_000613265.1_ASM61326v1\",\"GCA_000690815.1_ASM69081v1\",\"GCA_000714595.1_ASM71459v1\",\"GCA_000734955.1_ASM73495v1\",\"GCA_002007705.1_ASM200770v1\",\"GCA_003697165.2\",\"GCA_900092615.1_PRJEB14041\")\n",
    "\n",
    "for (file in file_list)\n",
    "{\n",
    "  file_path <- paste(file,'.faa',sep = '')\n",
    "  out_path <- paste(file,'_kmer_k5_matrix.csv',sep = '')\n",
    "  proteins <- read.FASTA(file_path, type = \"AA\")\n",
    "  print(paste('Genome ',file,' proteins are loaded'))\n",
    "  kmerCounts <- kcount(proteins, k = 5)\n",
    "  print(paste('Genome ',file, ' kmerCounts is finished'))\n",
    "  write.csv(as.matrix(kmerCounts), file = out_path)\n",
    "  print(paste('Genome ', file, ' matrix is output'))\n",
    "  print(paste('Genome ',file,' is complete', sep = ''))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* combine separate .csv files into one large .csv file for each genome (Multigenome_CSV_Generator.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CSV File Generator:\n",
    "import glob, pandas as pd\n",
    "data_folder = '/Users/matthewthompson/Documents/UAMS_SURF/K-mer_testing/FAA_files/10_genomes_4mer_csv/'\n",
    "file_list = glob.glob(data_folder + '*kmer_k4_matrix*')\n",
    "dicty = dict()\n",
    "for file in file_list:\n",
    "\n",
    "    df = pd.read_csv(file)\n",
    "    protein_id = list(df['Unnamed: 0'])\n",
    "    protein_id = [x.split(' ')[0] for x in protein_id]\n",
    "    df['protein_id'] = protein_id\n",
    "    df.set_index('protein_id',inplace = True)\n",
    "    del df['Unnamed: 0']\n",
    "    \n",
    "    df = df.T\n",
    "    \n",
    "    for col in list(df.columns):\n",
    "        loop = df[df[col] != 0]\n",
    "        loop = loop[col]\n",
    "        loop = loop.sort_values(ascending = False)\n",
    "        loop = loop[0:10]\n",
    "        dicty[col] = loop\n",
    "\n",
    "df_test = pd.DataFrame(dicty)\n",
    "\n",
    "df2 = df_test.fillna(0)\n",
    "df2.to_csv('/Users/matthewthompson/Documents/UAMS_SURF/K-mer_testing/FAA_files/10_genome_k4_kmer_top_10_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* read in combined .csv file and cluster genes based on cosine similarity of kmer profiles (canopyClusteringTest.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# X shoudl be a numpy matrix, very likely sparse matrix: http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix\n",
    "# T1 > T2 for overlapping clusters\n",
    "# T1 = Distance to centroid point to not include in other clusters\n",
    "# T2 = Distance to centroid point to include in cluster\n",
    "# T1 > T2 for overlapping clusters\n",
    "# T1 < T2 will have points which reside in no clusters\n",
    "# T1 == T2 will cause all points to reside in mutually exclusive clusters\n",
    "# Distance metric can be any from here: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html\n",
    "# filemap may be a list of point names in their order in X. If included, row numbers from X will be replaced with names from filemap. \n",
    " \n",
    "def canopy(X1_dist, T1, T2, filemap=None):\n",
    "    canopies = dict()\n",
    "    canopy_points = set(range(X1_dist.shape[0]))\n",
    "    grouping_list = [None] * X1_dist.shape[0]\n",
    "    iteration = 0\n",
    "    clustered_points = []\n",
    "    while canopy_points:\n",
    "        iteration = iteration + 1\n",
    "        print(\"iteration: \" + str(iteration))\n",
    "        point = canopy_points.pop()\n",
    "        i = len(canopies)\n",
    "        points_in_T2 = np.where(X1_dist[point] < T2)[0]\n",
    "        points_to_cluster = set(points_in_T2).difference(set(clustered_points))\n",
    "        canopies[i] = {\"c\":point, \"points\": points_to_cluster}\n",
    "        clustered_points.extend(points_to_cluster)\n",
    "        canopy_points = canopy_points.difference(set(np.where(X1_dist[point] < T1)[0]))\n",
    "        \n",
    "        for entry in canopies[i][\"points\"]:\n",
    "            grouping_list[entry] = point\n",
    "        \n",
    "    if filemap:\n",
    "        for canopy_id in canopies.keys():\n",
    "            canopy = canopies.pop(canopy_id)\n",
    "            canopy2 = {\"c\":filemap[canopy['c']], \"points\":list()}\n",
    "            for point in canopy['points']:\n",
    "                canopy2[\"points\"].append(filemap[point])\n",
    "            canopies[canopy_id] = canopy2\n",
    "    return grouping_list\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv('/Users/matthewthompson/Documents/UAMS_SURF/K-mer_testing/FAA_files/10_genome_k4_kmer_top_10_table.csv')\n",
    "    #gets protein_ids\n",
    "    kmers = list(df['Unnamed: 0'])\n",
    "    df['kmers'] = kmers\n",
    "    #move from data column to index column\n",
    "    df.set_index('kmers',inplace = True)\n",
    "    #delete protein header list\n",
    "    del df['Unnamed: 0']\n",
    "    df = df.T\n",
    "    df.head(3)\n",
    "    print(\"canopy clustering:\")\n",
    "    X1_dist = pairwise_distances(df.to_numpy(), metric='cosine')\n",
    "    \n",
    "    grouping_list = canopy(X1_dist, 0.99, 0.99)\n",
    "    \n",
    "    '''\n",
    "    with open('cosine_canopy_clusters_0.99.csv', 'w') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        for key, value in clusters.items():\n",
    "            writer.writerow([key, value])\n",
    "    '''\n",
    "    with open('grouping_list.csv', 'w') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(grouping_list) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I was able to import the groupings from the .csv file into R and use them as the groupings of a pangenome object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(FindMyFriends)\n",
    "library(kebabs)\n",
    "\n",
    "setwd(\"/Users/matthewthompson/Documents/UAMS_SURF/K-mer_testing/FAA_files/\")\n",
    "genomeFiles <- list.files(getwd(), full.names=TRUE, pattern='*.faa')\n",
    "pan <- pangenome(genomeFiles[1:10], translated=TRUE, geneLocation='prodigal', lowMem=FALSE)\n",
    "groupingList <- as.integer(read.csv(\"grouping_list.csv\", header=FALSE, sep=',', stringsAsFactors = FALSE))\n",
    "pan <- manualGrouping(pan, groupingList)\n",
    "plotStat(pan, type='qual', palette = 6)\n",
    "plotSimilarity(pan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I chose such a high threshold for the clustering step, I only had a small amount of groups for analysis. I was able to generate graphs, but I need to see how accurate they are. ![summ](images/w3/ManGroupSumm_0.99.png) ![sim](images/w3/ManGroupSim_0.99.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what I can tell by the groupings of the genomes that Kaleb sent, this grouping seems too coarse. Last week's heatmap grouped similar phylotypes together, but they are all jumbled in this heatmap. It is likely due to the clustering threshold, so I will have to tweak that later. This just serves as a great proof of concept that we can pre-cluster the data and then successfully get it into FindMyFriends!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wednesday June 5, 2019:\n",
    "My first goal today is to modify my clustering algorithm to separate out paralogues (duplicate genes in the same genome) into different groups. We are aiming for approximately one group per protein, and each orthologous protein from each strain should hopefully end up in the same cluster. My code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# X shoudl be a numpy matrix, very likely sparse matrix: http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix\n",
    "# T1 > T2 for overlapping clusters\n",
    "# T1 = Distance to centroid point to not include in other clusters\n",
    "# T2 = Distance to centroid point to include in cluster\n",
    "# T1 > T2 for overlapping clusters\n",
    "# T1 < T2 will have points which reside in no clusters\n",
    "# T1 == T2 will cause all points to reside in mutually exclusive clusters\n",
    "# Distance metric can be any from here: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html\n",
    "# filemap may be a list of point names in their order in X. If included, row numbers from X will be replaced with names from filemap. \n",
    " \n",
    "def main():\n",
    "    # read in and format data from .csv file\n",
    "    df = pd.read_csv('/Users/matthewthompson/Documents/UAMS_SURF/K-mer_testing/CSV_files/10_genome_k4_kmer_top_10_table.csv')\n",
    "    kmers = list(df['Unnamed: 0'])\n",
    "    df['kmers'] = kmers\n",
    "    df.set_index('kmers',inplace = True)\n",
    "    del df['Unnamed: 0']\n",
    "    df = df.T\n",
    "    df.head(3)\n",
    "    \n",
    "    # calculate and format distance matrix\n",
    "    X1_dist = pd.DataFrame(pairwise_distances(df, metric='cosine'))\n",
    "    X1_dist.columns = df.index\n",
    "    X1_dist.index = df.index\n",
    "    \n",
    "    # distance threshold for clustering proteins together\n",
    "    threshold = 0.5\n",
    "    \n",
    "    # set up and format data structures for algorithm\n",
    "    canopies = dict()\n",
    "    elligible_points = list(X1_dist.columns)\n",
    "    points_in_threshold = []\n",
    "    clustered_points = []\n",
    "    \n",
    "    # set up and format final grouping_list \n",
    "    grouping_list = pd.DataFrame(list([None] * X1_dist.shape[0]))\n",
    "    grouping_list = grouping_list.T\n",
    "    grouping_list.columns = X1_dist.columns\n",
    "\n",
    "    iteration = 0\n",
    "    while len(elligible_points) > 0:\n",
    "        iteration = iteration + 1\n",
    "        print(\"iteration: \" + str(iteration))\n",
    "        # record which organisms' proteins have been clustered into this cluster\n",
    "        # only one protein per organism is allowed in this cluster\n",
    "        clustered_organisms = []\n",
    "\n",
    "        center_point = list(elligible_points)[-1]\n",
    "        i = len(canopies)\n",
    "        \n",
    "        for point in elligible_points:\n",
    "            if(X1_dist[center_point][point] < threshold):\n",
    "                # check for organisms that have already been clustered\n",
    "                if(point.split('_')[0] not in clustered_organisms):\n",
    "                    points_in_threshold.append(point) \n",
    "                    clustered_organisms.append(point.split('_')[0])\n",
    "                    \n",
    "        # cluster points which have not already been clustered\n",
    "        points_to_cluster = set(points_in_threshold).difference(set(clustered_points))\n",
    "        clustered_points.extend(points_to_cluster)\n",
    "        elligible_points = set(elligible_points).difference(set(points_to_cluster))\n",
    "        \n",
    "        canopies[i] = {\"c\":iteration, \"points\": points_to_cluster}\n",
    "        \n",
    "        for entry in canopies[i][\"points\"]:\n",
    "            grouping_list[entry] = iteration\n",
    "\n",
    "    with open('/Users/matthewthompson/Documents/UAMS_SURF/K-mer_testing/CSV_files/10_genome_top_10_4mer_cosine_canopy_clusters_0.5_ps.csv', 'w') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        for key, value in canopies.items():\n",
    "            writer.writerow([key, value])\n",
    "    \n",
    "    with open('/Users/matthewthompson/Documents/UAMS_SURF/K-mer_testing/CSV_files/10_genome_top_10_4mer_grouping_list_0.5_ps.csv', 'w') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(list(grouping_list.iloc[0])) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I got this code working, I then got the pangeome results using FindMyFriends and compared the results when paralogues were grouped against when paralogues were separated. My results: ![paraComparison](images/w3/paraloguesComparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then investigated the FindMyFriends (1.1.6) code to see how it grouped the gene groups (into Core, Accessory, Singleton), and I found that groups were only classified as Core when the gene group had a gene from every single organism. I then lowered the threshold for Core designation to be 90% for shared samples, and I got a larger core section for the pangenome: ![90core](images/w3/10_genome_top_10_4mers_0.5_ps_0.9_core.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
